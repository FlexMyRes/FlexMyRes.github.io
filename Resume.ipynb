{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from github import Github\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "#initialize matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads into list after applying nlp to each word\n",
    "def read_pdf(fileName):\n",
    "        f = PyPDF2.PdfFileReader(fileName)\n",
    "        f2=f.getPage(0)\n",
    "        text = str(f2.extractText()).replace('\\n',' ')\n",
    "        return text\n",
    "\n",
    "#reads txt files\n",
    "def read_txt(fileName):\n",
    "    fileObj = open(fileName, \"r\") #opens the file in read mode\n",
    "    words = fileObj.read().replace(\"\\n\", \" \") #puts the file into an array\n",
    "    fileObj.close()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', None, pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills2(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    hard_data = nlp(read_txt('hard.txt'))\n",
    "    hard_tokens = [token.text for token in hard_data]\n",
    "    hard_skills=[]\n",
    "    \n",
    "    soft_data = nlp(read_txt('soft.txt'))\n",
    "    soft_tokens = [token.text for token in soft_data]\n",
    "    soft_skills=[]\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in hard_data:\n",
    "        for token2 in nlp_text.noun_chunks:\n",
    "            if token.similarity(token2)>0.8:\n",
    "                hard_skills.append(token)\n",
    "                \n",
    "    for token in soft_data:\n",
    "        for token2 in nlp_text.noun_chunks:\n",
    "            if token.similarity(token2)>0.7:\n",
    "                soft_skills.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    return hard_skills,soft_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hardsoft(hard,soft):\n",
    "    hard_skills = pd.DataFrame(hard)\n",
    "    soft_skills = pd.DataFrame(soft)\n",
    "    hard_skills[0] = hard_skills[0].astype(str).str.strip()\n",
    "    soft_skills[0] = soft_skills[0].astype(str).str.strip()\n",
    "    return hard_skills[0].value_counts(),soft_skills[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to run in this order\n",
    "## text=read_pdf(file)\n",
    "\n",
    "#finding user name\n",
    "## extract_name(text)\n",
    "\n",
    "#get skills\n",
    "##hard,soft = extract_skills2(text)\n",
    "\n",
    "#get values to plot \n",
    "## hard_set,soft_set=graph_hardsoft(hard,soft)\n",
    "\n",
    "##TYPE THIS IN MANUALLY-- NOT A FUNCTION\n",
    "### code to run manually to graph the above responses\n",
    "#hard_plot = hard_skills[0].value_counts().plot.pie(subplots=True)\n",
    "#soft_plot = soft_skills[0].value_counts().plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
